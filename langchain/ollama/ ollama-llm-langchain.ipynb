{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import OllamaLLM, ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "import psycopg\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, POST, N3\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, OWL, FOAF, XSD, SKOS, DCTERMS\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import urllib.parse\n",
    "from typing import List, Tuple, Dict, Set, DefaultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)  # Change to WARNING or ERROR in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langain & Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check ollama status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl --location 'http://127.0.0.1:11434/api/generate' \\\n",
    "# --header 'Content-Type: application/json' \\\n",
    "# --data '{ \\\n",
    "#     \"model\": \"llama3.2:3b\", \\\n",
    "#     \"prompt\": \"hello llama!\",  \\\n",
    "#     \"options\": { \\\n",
    "#         \"temperature\": 0 \\\n",
    "#     } \\\n",
    "# }' \\\n",
    "# | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://localhost:11434/api/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama model: configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0)\n",
    "# llm = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "chat_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: invoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(input=\"tell me a joke\")\n",
    "response = llm.invoke(\"hello ollama!\")\n",
    "\n",
    "# response = llm.invoke(\"Create an agent that uses Ollama function calling in Langchain.\")\n",
    "\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "chat_ollama.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = chat_prompt_template | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template & StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt_template | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vector store & a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. select a specfic datasource. In this case a web page.\n",
    "# 2. save extracted content from the web page as docs.\n",
    "# 3. index the docs using FAISS vector store.\n",
    "# 4. convert the vector store to retriever.\n",
    "\n",
    "web_base_loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = web_base_loader.load()\n",
    "\n",
    "# print(f\"type(docs) : {type(docs)} \\n\")\n",
    "# print(f\"len(docs) : {len(docs)}\\n\")\n",
    "# print(f\"docs: {docs} \\n\")\n",
    "# type(f\"docs[0] : {docs[0]} \\n\")\n",
    "# print(f\"docs[0].page_content : {docs[0].page_content} \\n\")\n",
    "\n",
    "recursive_character_text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = recursive_character_text_splitter.split_documents(documents=docs)\n",
    "\n",
    "\n",
    "# print(type(documents))\n",
    "# print(len(documents))\n",
    "# print(documents)\n",
    "# print(documents[0])\n",
    "# print(documents[2])\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents, embedding=ollama_embedding)\n",
    "\n",
    "\n",
    "# print(f\"vector_store.index.ntotal: {vector_store.index.ntotal}\")\n",
    "# print(f\"vector_store._get_retriever_tags() : {vector_store._get_retriever_tags()}\")\n",
    "# print(f\"vector_store.index_to_docstore_id : {vector_store.index_to_docstore_id}\")\n",
    "# print(f\"type(vector_store.index_to_docstore_id) : {type(vector_store.index_to_docstore_id)}\")\n",
    "\n",
    "vector_store_retriever = vector_store.as_retriever()\n",
    "print(f\"vector_store_retriever: {vector_store_retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create a chat prompt template\n",
    "# 6. create a stuff document chain that accepts a llm model and chat prompt template & we can also run stuff document chain by passing in documents directly\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(\n",
    "    llm=llm, prompt=chat_prompt_template)\n",
    "response = documents_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"how can langsmith help with testing?\",\n",
    "        \"context\": documents\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. create a document retrieval chain that takes vector store retriever and stuff document chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    vector_store_retriever, documents_chain)\n",
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "# print(type(response))\n",
    "pprint.pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "history_aware_retriever_chain = create_history_aware_retriever(\n",
    "    llm, vector_store_retriever, chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, chat_prompt_template)\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    history_aware_retriever_chain, document_chain)\n",
    "\n",
    "chat_history = [HumanMessage(\n",
    "    content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"tell me how\"\n",
    "})\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_embedding = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")\n",
    "# ollama_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "ollama_embedding = OllamaEmbeddings(model=\"bge-m3:567m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: postgresql+psycopg2://user:password@host:port/dbname\n",
    "# Database Connection Details\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "COLLECTION_NAME = \"dbpedia_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Connecting to PGVector 'dbpedia_docs'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfull!\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nConnecting to PGVector '{COLLECTION_NAME}'...\")\n",
    "try:\n",
    "    # If the collection table doesn't exist, PGVector will try to create it.\n",
    "    vectorstore = PGVector(\n",
    "        connection=CONNECTION_STRING,\n",
    "        embeddings=ollama_embedding,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        use_jsonb=True\n",
    "        # pre_delete_collection=True\n",
    "        # Use pre_delete_collection=True if you want to clear the collection on every run (USE WITH CAUTION!)\n",
    "        # pre_delete_collection=False,\n",
    "    )\n",
    "    print(f\"connection successfull!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    logger.exception(f\"\\nDatabase Connection Error: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.exception(f\"\\nAn error occurred during PGVector connection: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to ontotext graph db and fetch all the entities and the description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHDB_BASE_URL = os.getenv(\"GRAPHDB_BASE_URL\")\n",
    "GRAPHDB_REPOSITORY = os.getenv(\"GRAPHDB_REPOSITORY\")\n",
    "\n",
    "# Format: {base_url}/repositories/{repository_id}\n",
    "SPARQL_ENDPOINT = urljoin(GRAPHDB_BASE_URL.strip('/') + '/', f\"repositories/{GRAPHDB_REPOSITORY}\")\n",
    "\n",
    "OUTPUT_FILENAME_DIR = os.path.join(\"c:\\\\Users\\\\deepa\\\\data\\\\workspace\\\\notebooks\", \"datasets\", \"instance_description\")\n",
    "OUTPUT_FILENAME = os.path.join(OUTPUT_FILENAME_DIR, \"instance_description.jsonl\")\n",
    "\n",
    "FAILED_LOG_DIR = os.path.join(\"c:\\\\Users\\\\deepa\\\\data\\\\workspace\\\\notebooks\", \"datasets\", \"failed\")\n",
    "FAILED_CLASS_LOG = os.path.join(FAILED_LOG_DIR, \"failed_class_iri.txt\")\n",
    "FAILED_INSTANCE_LOG = os.path.join(FAILED_LOG_DIR, \"failed_instance_iri.txt\")\n",
    "\n",
    "\n",
    "INSTANCE_BATCH_SIZE = 100\n",
    "DELAY_BETWEEN_BATCHES = 1.0\n",
    "DELAY_BETWEEN_CLASSES = 2.0 # Only used when iterating classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_WORKERS = os.cpu_count()\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "fail_class_file_lock = threading.Lock()\n",
    "fail_instance_file_lock = threading.Lock()\n",
    "output_file_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql(return_format=JSON):\n",
    "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "    sparql.setReturnFormat(return_format)\n",
    "    return sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_classes():\n",
    "    \n",
    "    logger.info(\"Fetching ontology classes from model graph\")\n",
    "\n",
    "    class_query = r\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "    SELECT ?class\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    WHERE {\n",
    "      ?class a owl:Class .\n",
    "      FILTER (\n",
    "        regex(STRAFTER(STR(?class), \"http://dbpedia.org/ontology/\"), \"^[\\\\x00-\\\\x7F]+$\")\n",
    "      )\n",
    "    }\n",
    "    ORDER BY ?class\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sparql = get_sparql(return_format=JSON)\n",
    "        sparql.setQuery(class_query)\n",
    "        results = sparql.query().convert()\n",
    "        classes = [result[\"class\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
    "        logger.info(f\"Fetched {len(classes)} classes.\")\n",
    "        return classes\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"[Error] Fetching classes: {type(e).__name__} - {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Instances of a particular Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_instances_for_class(ontology_class):\n",
    "    \n",
    "    logger.info(f\"Fetching instances of type class {ontology_class} from data graph\")\n",
    "\n",
    "    instance_query = f\"\"\"\n",
    "    SELECT ?instance\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    FROM <http://dbpedia.org/data>\n",
    "    WHERE {{\n",
    "        BIND(<{ontology_class}> AS ?entity)\n",
    "        ?instance a ?entity .\n",
    "    }}\n",
    "    ORDER BY ?instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sparql = get_sparql(return_format=JSON)\n",
    "        sparql.setQuery(instance_query)\n",
    "        results = sparql.query().convert()\n",
    "        instances = [result[\"instance\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
    "        return instances\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"[Error] Fetching instances for {ontology_class}: {type(e).__name__} - {e}\")\n",
    "        try:\n",
    "            with fail_class_file_lock:\n",
    "                with open(FAILED_CLASS_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(ontology_class + \"\\n\")\n",
    "        except Exception as file_err:\n",
    "            logger.exception(f\"[Error] Saving failed class IRI to {FAILED_CLASS_LOG}: {file_err}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Describe a single instance\n",
    "def describe_instance(instance_iri):\n",
    "    \n",
    "    logger.info(f\"Describing instance {instance_iri} from data graph\")\n",
    "\n",
    "    query = f\"DESCRIBE <{instance_iri}>\"\n",
    "    try:\n",
    "        sparql = get_sparql(return_format=N3)\n",
    "        sparql.setQuery(query)\n",
    "        result_bytes = sparql.query().convert()\n",
    "        rdf_n3_string = result_bytes.decode('utf-8') if isinstance(result_bytes, bytes) else str(result_bytes)\n",
    "        return rdf_n3_string\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"[Error] Describing {instance_iri}: {type(e).__name__} - {e}\")\n",
    "        try:\n",
    "            with fail_instance_file_lock:\n",
    "                with open(FAILED_INSTANCE_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(instance_iri + \"\\n\")\n",
    "        except Exception as file_err:\n",
    "            logger.exception(f\"[Error] Saving failed instance IRI to {FAILED_INSTANCE_LOG}: {file_err}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_iri = \"http://dbpedia.org/resource/Roger_Federer\"\n",
    "# response = describe_instance(instance_iri)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tranform describe output to Key-Value and Tuple format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_uri(uri_str: str) -> str:\n",
    "    \"\"\"Extracts a human-readable label from a URI string.\"\"\"\n",
    "    if not isinstance(uri_str, str) or not (uri_str.startswith('<') and uri_str.endswith('>')):\n",
    "        return str(uri_str) # Return as is if not a valid URI string\n",
    "\n",
    "    uri = uri_str.strip('<>')\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(uri)\n",
    "        if parsed.fragment:\n",
    "            label_part = parsed.fragment\n",
    "        else:\n",
    "            path_parts = [part for part in parsed.path.split('/') if part]\n",
    "            label_part = path_parts[-1] if path_parts else uri\n",
    "\n",
    "        decoded_label = urllib.parse.unquote(label_part)\n",
    "        # Replace common separators with spaces\n",
    "        label = decoded_label.replace('_', ' ').replace('-', ' ')\n",
    "        # Basic CamelCase to space separation\n",
    "        label = re.sub(r'(?<!^)(?=[A-Z])', ' ', label)\n",
    "        # *** ADDED: Replace multiple spaces with a single space and strip ***\n",
    "        label = re.sub(r'\\s+', ' ', label).strip()\n",
    "        return label if label else label_part # Return original part if label becomes empty\n",
    "\n",
    "    except Exception:\n",
    "        return uri # Fallback to the raw URI on parsing errors\n",
    "\n",
    "# --- clean_value function remains the same ---\n",
    "def clean_value(rdf_term: str) -> str:\n",
    "    \"\"\"Cleans an RDF term (URI or Literal), returning a readable string.\"\"\"\n",
    "    # Expects rdf_term to already be stripped of leading/trailing whitespace\n",
    "    if rdf_term.startswith('<') and rdf_term.endswith('>'):\n",
    "        return get_label_from_uri(rdf_term)\n",
    "    elif rdf_term.startswith('\"'):\n",
    "        match = re.match(r'\"(.*?)\"', rdf_term)\n",
    "        return match.group(1) if match else rdf_term\n",
    "    elif rdf_term.startswith('_:'):\n",
    "        return rdf_term\n",
    "    return rdf_term\n",
    "\n",
    "# --- process_n3_simplified function remains the same ---\n",
    "def process_n3_simplified(n3_data: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes N3 triples generically, identifies the main subject,\n",
    "    and returns a formatted key-value summary with lowercase predicate labels,\n",
    "    grouping outgoing and incoming relationships.\n",
    "\n",
    "    Args:\n",
    "        n3_data: A string containing N3 triples, one per line.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string summarizing the main entity and its relationships.\n",
    "    \"\"\"\n",
    "    triples: List[Tuple[str, str, str]] = []\n",
    "    uri_subjects: List[str] = []\n",
    "\n",
    "    # 1. Parse Triples\n",
    "    triple_pattern = re.compile(r'^\\s*(<[^>]+>|_:\\S+)\\s+(<[^>]+>)\\s+(.*)\\s*\\.\\s*$')\n",
    "    for line in n3_data.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        match = triple_pattern.match(line)\n",
    "        if match:\n",
    "            s, p, o_raw = match.groups()\n",
    "            triples.append((s, p, o_raw))\n",
    "            if s.startswith('<'):\n",
    "                uri_subjects.append(s)\n",
    "        else:\n",
    "            print(f\"Warning: Skipping malformed line: {line}\")\n",
    "\n",
    "    if not triples:\n",
    "        return \"No valid triples found.\"\n",
    "    if not uri_subjects:\n",
    "        return \"No URI subjects found to determine a main entity.\"\n",
    "\n",
    "    # 2. Identify Main Subject\n",
    "    subject_counts = Counter(uri_subjects)\n",
    "    main_subject_uri = subject_counts.most_common(1)[0][0]\n",
    "    main_subject_iri = main_subject_uri.strip('<>')\n",
    "    derived_main_label = get_label_from_uri(main_subject_uri)\n",
    "\n",
    "    # 3. Process Information Generically\n",
    "    properties: DefaultDict[str, Set[str]] = defaultdict(set)\n",
    "    incoming: Set[Tuple[str, str, str]] = set()\n",
    "\n",
    "    for s, p, o_raw in triples:\n",
    "        predicate_label = get_label_from_uri(p).lower()\n",
    "        o_stripped = o_raw.strip()\n",
    "\n",
    "        if s == main_subject_uri:\n",
    "            object_value = clean_value(o_stripped)\n",
    "            properties[predicate_label].add(object_value)\n",
    "        elif o_stripped == main_subject_uri:\n",
    "            subj_label = clean_value(s)\n",
    "            incoming.add((subj_label, predicate_label, derived_main_label))\n",
    "\n",
    "    # 4. Format Output\n",
    "    output_lines = []\n",
    "    output_lines.append(f\"IRI: {main_subject_iri}\")\n",
    "\n",
    "    explicit_label = None\n",
    "    common_label_preds_lower = ['label', 'rdfs label', 'skos pref label', 'name']\n",
    "    for label_pred in common_label_preds_lower:\n",
    "         if label_pred in properties:\n",
    "             explicit_label = sorted(list(properties[label_pred]))[0]\n",
    "             if explicit_label:\n",
    "                 # properties.pop(label_pred, None) # Optional: remove from outgoing\n",
    "                 break\n",
    "\n",
    "    output_lines.append(f\"label: {explicit_label if explicit_label else derived_main_label}\")\n",
    "\n",
    "    if properties:\n",
    "        output_lines.append(\"\\nOutgoing Relationships:\")\n",
    "        for predicate_label in sorted(properties.keys()):\n",
    "            sorted_values = sorted(list(properties[predicate_label]))\n",
    "            output_lines.append(f\"{predicate_label}: {', '.join(sorted_values)}\")\n",
    "\n",
    "    if incoming:\n",
    "        output_lines.append(\"\\nIncoming Relationships:\")\n",
    "        sorted_incoming = sorted(list(incoming))\n",
    "        for subj_l, pred_l, obj_l in sorted_incoming:\n",
    "            output_lines.append(f\"({subj_l}, {pred_l}, {obj_l})\")\n",
    "\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = process_n3_simplified(response)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instance_worker(instance_iri: str):\n",
    "    try:\n",
    "        logger.debug(f\"Worker processing instance: {instance_iri}\")\n",
    "        n3_data = describe_instance(instance_iri) # This function handles its own failure logging\n",
    "\n",
    "        if n3_data is not None:\n",
    "            processed_description = process_n3_simplified(n3_data)\n",
    "            output_data = {\n",
    "                \"iri\": instance_iri,\n",
    "                \"description\": processed_description\n",
    "            }\n",
    "            try:\n",
    "                with output_file_lock:\n",
    "                    with open(OUTPUT_FILENAME, \"a\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(output_data, f)\n",
    "                        f.write(\"\\n\")\n",
    "                logger.debug(f\"Successfully processed and saved: {instance_iri}\")\n",
    "            except Exception as file_err:\n",
    "                logger.exception(f\"[Error] Writing output for {instance_iri} to {OUTPUT_FILENAME}: {file_err}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"[Error] Unexpected error in worker for instance {instance_iri}: {e}\")\n",
    "        try:\n",
    "            with fail_instance_file_lock:\n",
    "                with open(FAILED_INSTANCE_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(instance_iri + \"\\n\")\n",
    "        except Exception as file_err:\n",
    "            logger.exception(f\"[Error] Saving failed instance IRI (worker error) to {FAILED_INSTANCE_LOG}: {file_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    os.makedirs(OUTPUT_FILENAME_DIR, exist_ok=True)\n",
    "    os.makedirs(FAILED_LOG_DIR, exist_ok=True)\n",
    "\n",
    "    classes = fetch_classes()\n",
    "    if not classes:\n",
    "        logger.warning(\"No classes fetched. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_instances_processed = 0\n",
    "    for i, ontology_class in enumerate(classes):\n",
    "        logger.info(f\"Processing class {i+1}/{len(classes)}: {ontology_class} ---\")\n",
    "        instances = fetch_instances_for_class(ontology_class)\n",
    "\n",
    "        if not instances:\n",
    "            logger.warning(f\"No instances found or fetch failed for class {ontology_class}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Submitting {len(instances)} instances of class {ontology_class} for processing...\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_instance_worker, instance_iri)\n",
    "                for instance_iri in instances\n",
    "            ]\n",
    "\n",
    "            # Wait for all tasks for this class to complete and log progress\n",
    "            processed_count = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                processed_count += 1\n",
    "                # Log progress periodically if needed\n",
    "                if processed_count % 100 == 0:\n",
    "                     logger.info(f\"Class {ontology_class}: Processed {processed_count}/{len(instances)} instances...\")\n",
    "                try:\n",
    "                    # Check if the future raised an exception (already logged in worker)\n",
    "                    future.result() # Raises exception if one occurred in the worker task\n",
    "                except Exception:\n",
    "                    # Exception already logged by the worker or describe_instance\n",
    "                    pass # Or add specific handling here if needed\n",
    "\n",
    "        total_instances_processed += len(instances) # Count submitted instances\n",
    "        logger.info(f\"Finished processing instances for class {ontology_class}.\")\n",
    "\n",
    "    logger.info(\"Instance description process completed\")\n",
    "    logger.info(f\"Total classes processed: {len(classes)}\")\n",
    "    logger.info(f\"Total instances submitted for processing: {total_instances_processed}\")\n",
    "\n",
    "    try:\n",
    "        with open(FAILED_INSTANCE_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "            failed_instance_count = sum(1 for _ in f)\n",
    "        logger.info(f\"Total instances failed during description/processing (check log): {failed_instance_count}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"No instance failures logged.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not read failed instance log count: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(FAILED_CLASS_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "            failed_class_count = sum(1 for _ in f)\n",
    "        logger.info(f\"Total classes failed during instance fetching (check log): {failed_class_count}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"No class failures logged.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not read failed class log count: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the entity Description from the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_jsonl(filename):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file line by line and yields each parsed JSON object.\n",
    "    This allows processing one record at a time without loading the whole file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_number, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    yield json.loads(line) # Yield the parsed dictionary\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON on line {line_number} in {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filename}: {e}\")\n",
    "\n",
    "# Example Usage:\n",
    "for record in read_one_jsonl(OUTPUT_FILENAME):\n",
    "    print(\"---\"*50)\n",
    "    print(f\"IRI: {record.get('iri')}\")\n",
    "    print(f\"Description: {record.get('description')}\")\n",
    "    # Process the record here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the following information about the entity http://dbpedia.org/resource/100_Word_Story:\n",
    "\n",
    "# Name: 100 Word Story\n",
    "# Abbreviation: 100 Word Story\n",
    "# Type: Academic Journal, Periodical Literature, Written Work, Creative Work\n",
    "# First published in: 2011\n",
    "# Frequency of publication: Quarterly\n",
    "# Academic discipline: Literary Magazine\n",
    "# Editor: Grant Faulkner\n",
    "# Homepage: http://www.100wordstory.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ollama-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
