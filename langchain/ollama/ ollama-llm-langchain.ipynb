{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import OllamaLLM, ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "import psycopg\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, POST, N3\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, OWL, FOAF, XSD, SKOS, DCTERMS\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langain & Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check ollama status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl --location 'http://127.0.0.1:11434/api/generate' \\\n",
    "# --header 'Content-Type: application/json' \\\n",
    "# --data '{ \\\n",
    "#     \"model\": \"llama3.2:3b\", \\\n",
    "#     \"prompt\": \"hello llama!\",  \\\n",
    "#     \"options\": { \\\n",
    "#         \"temperature\": 0 \\\n",
    "#     } \\\n",
    "# }' \\\n",
    "# | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://localhost:11434/api/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama model: configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0)\n",
    "# llm = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "chat_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: invoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# llm.invoke(input=\"tell me a joke\")\n",
    "response = llm.invoke(\"hello ollama!\")\n",
    "\n",
    "# response = llm.invoke(\"Create an agent that uses Ollama function calling in Langchain.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je aime programmer.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-04-14T19:07:45.0711592Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2673019600, 'load_duration': 2457519600, 'prompt_eval_count': 42, 'prompt_eval_duration': 153058000, 'eval_count': 5, 'eval_duration': 57947000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-87d0096c-961f-4def-8049-07069bb59937-0', usage_metadata={'input_tokens': 42, 'output_tokens': 5, 'total_tokens': 47})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "chat_ollama.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how LangSmith can help with testing. I remember that LangSmith is some kind of AI tool related to language processing, maybe for writing or something like that. But I'm not exactly sure about its specific features beyond generating text.\n",
      "\n",
      "The user mentioned testing in their question, so I guess they're asking if LangSmith has any features that assist in testing processes. Testing can be a broad term‚Äîlike software testing, quality assurance, user acceptance testing, etc.‚Äîso I need to think about how an AI tool like LangSmith might fit into these contexts.\n",
      "\n",
      "First, maybe LangSmith can help with automated testing. If it's capable of generating text based on inputs, perhaps it can create test cases or scenarios automatically. That would save time compared to manual testing. But I'm not sure if LangSmith has that feature or not.\n",
      "\n",
      "Another angle is using LangSmith for functional testing. If you're testing a system's functionality, maybe LangSmith can simulate user interactions or generate expected outputs, helping to verify if the system behaves as intended. For example, if you have a chatbot, LangSmith could be used to test its responses by generating various inputs and checking if the outputs meet expectations.\n",
      "\n",
      "Then there's integration testing. If LangSmith can interact with other systems or APIs, it might help in testing how different components work together. But again, I'm not certain about LangSmith's capabilities in that area.\n",
      "\n",
      "Testing for natural language processing models could also be a use case. Maybe LangSmith can generate test cases or examples to evaluate the performance of NLP systems. This would involve creating scenarios where the system should perform well and then checking if it does.\n",
      "\n",
      "I also wonder about testing in the context of localization‚Äîensuring that software works correctly in different languages. LangSmith might help by generating localized text for testing purposes, ensuring that translations are accurate and consistent.\n",
      "\n",
      "Another thought is about performance testing. If LangSmith can simulate multiple users or requests, it could help stress-test a system to see how it handles high loads. This would be useful for scalability testing.\n",
      "\n",
      "Documentation testing is another possibility. If LangSmith can generate documentation, maybe it can also test that documentation by checking for accuracy and completeness. But I'm not sure if that's a feature it offers.\n",
      "\n",
      "I should also consider the user interface aspects of testing. Maybe LangSmith can assist in testing how an application presents information to users, ensuring usability and accessibility.\n",
      "\n",
      "Wait, but I might be mixing up different tools here. I know about some AI tools that help with code testing or static analysis, but Langsmith seems more focused on text generation. So perhaps its role is more in generating test scripts or scenarios rather than executing them.\n",
      "\n",
      "In any case, the key points would likely involve using LangSmith to automate parts of the testing process, generate test cases, simulate user interactions, and possibly integrate with other testing frameworks. It might also help in validating outputs against expected results, especially for text-based systems.\n",
      "\n",
      "I should structure this into clear sections, maybe like automated testing, functional testing, integration testing, NLP model testing, localization testing, performance testing, documentation testing, and UI/UX testing. Each section would explain how LangSmith could assist in that specific area.\n",
      "\n",
      "But I'm not entirely sure about all these points, so I might need to check if LangSmith actually has these features or if they're more theoretical possibilities based on its general capabilities as a language processing tool.\n",
      "</think>\n",
      "\n",
      "LangSmith can be effectively utilized across various testing scenarios through its text generation and processing capabilities. Here's how it can assist:\n",
      "\n",
      "1. **Automated Testing**: LangSmith can generate test cases automatically, reducing manual effort and enhancing efficiency in creating scenarios for testing.\n",
      "\n",
      "2. **Functional Testing**: It can simulate user interactions by generating inputs and expected outputs, helping to verify system functionality, especially useful for chatbots or interactive systems.\n",
      "\n",
      "3. **Integration Testing**: If capable of interacting with other systems or APIs, LangSmith could assist in testing component interactions, ensuring seamless integration.\n",
      "\n",
      "4. **NLP Model Testing**: LangSmith can generate test cases to evaluate NLP models, checking their performance against expected outputs.\n",
      "\n",
      "5. **Localization Testing**: It can aid in generating localized text for testing, ensuring accuracy and consistency across different languages.\n",
      "\n",
      "6. **Performance Testing**: By simulating multiple users or requests, LangSmith can help stress-test systems to assess scalability under high loads.\n",
      "\n",
      "7. **Documentation Testing**: While not explicitly stated, LangSmith might assist in generating and testing documentation for accuracy and completeness.\n",
      "\n",
      "8. **UI/UX Testing**: It could evaluate how information is presented to users, ensuring usability and accessibility.\n",
      "\n",
      "In summary, LangSmith's role in testing likely involves automating test generation, simulating interactions, and validating outputs across various system types, enhancing efficiency and effectiveness in the testing process.\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = chat_prompt_template | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template & StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how LangSmith can help with testing. I remember that LangSmith is some kind of AI tool related to language processing, maybe for writing or something like that. But I'm not exactly sure about its features beyond generating text.\n",
      "\n",
      "The user mentioned testing in their question, so I guess they're asking if LangSmith can be used for testing purposes. Hmm, how does that work? Well, testing usually involves checking if a system works as expected, right? So maybe LangSmith can help test other AI systems or applications?\n",
      "\n",
      "Wait, but LangSmith is more about generating text. Maybe it's used to create test cases or scenarios for testing something else. Or perhaps it can simulate user interactions to see how well another system responds. That could be useful for testing chatbots or other language-based applications.\n",
      "\n",
      "Another thought: maybe LangSmith can help in automating tests. Like, if you have a lot of test cases, LangSmith could generate the necessary inputs or expected outputs automatically. Or perhaps it can analyze the results of tests to find bugs or issues in the system under test.\n",
      "\n",
      "I'm also thinking about how LangSmith might assist in testing its own capabilities. For example, if someone is developing an AI model, they might use LangSmith to create varied inputs and see how well their model performs. That could help in identifying weaknesses or areas for improvement.\n",
      "\n",
      "But I'm not entirely sure if LangSmith has specific features built for testing. It might just be that it's a versatile tool that can be adapted for testing purposes by generating test data, simulating user interactions, or automating repetitive test scenarios.\n",
      "\n",
      "I should also consider the possible limitations. Maybe LangSmith isn't designed specifically as a testing tool, so its capabilities in this area might be limited compared to dedicated testing tools. But it could still offer some helpful functionalities for certain aspects of testing.\n",
      "\n",
      "Overall, I think LangSmith can help with testing by providing generated text for test cases, simulating user interactions, or automating parts of the testing process. It's probably most useful in scenarios where generating varied inputs or expected outputs is needed to test a system effectively.\n",
      "</think>\n",
      "\n",
      "LangSmith can assist with testing in several ways:\n",
      "\n",
      "1. **Test Case Generation**: It can generate diverse and complex text inputs for testing systems, helping to create varied scenarios for evaluation.\n",
      "\n",
      "2. **Simulation of User Interactions**: By producing realistic conversations or interactions, LangSmith can simulate user behavior, aiding in the testing of chatbots or language-based applications.\n",
      "\n",
      "3. **Automated Testing Processes**: It may help automate repetitive tasks in testing, such as creating test scripts or analyzing results, though this might be more indirect compared to dedicated tools.\n",
      "\n",
      "4. **Internal Testing Capabilities**: Developers can use LangSmith to test their own AI models by generating inputs and assessing performance, aiding in the identification of weaknesses.\n",
      "\n",
      "While not a dedicated testing tool, LangSmith's text generation capabilities make it adaptable for various testing needs, particularly where generating test data or simulating interactions is beneficial.\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt_template | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vector store & a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_retriever: tags=['FAISS', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000233810D3DD0> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "# 1. select a specfic datasource. In this case a web page.\n",
    "# 2. save extracted content from the web page as docs.\n",
    "# 3. index the docs using FAISS vector store.\n",
    "# 4. convert the vector store to retriever.\n",
    "\n",
    "web_base_loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = web_base_loader.load()\n",
    "\n",
    "# print(f\"type(docs) : {type(docs)} \\n\")\n",
    "# print(f\"len(docs) : {len(docs)}\\n\")\n",
    "# print(f\"docs: {docs} \\n\")\n",
    "# type(f\"docs[0] : {docs[0]} \\n\")\n",
    "# print(f\"docs[0].page_content : {docs[0].page_content} \\n\")\n",
    "\n",
    "recursive_character_text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = recursive_character_text_splitter.split_documents(documents=docs)\n",
    "\n",
    "\n",
    "# print(type(documents))\n",
    "# print(len(documents))\n",
    "# print(documents)\n",
    "# print(documents[0])\n",
    "# print(documents[2])\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents, embedding=ollama_embedding)\n",
    "\n",
    "\n",
    "# print(f\"vector_store.index.ntotal: {vector_store.index.ntotal}\")\n",
    "# print(f\"vector_store._get_retriever_tags() : {vector_store._get_retriever_tags()}\")\n",
    "# print(f\"vector_store.index_to_docstore_id : {vector_store.index_to_docstore_id}\")\n",
    "# print(f\"type(vector_store.index_to_docstore_id) : {type(vector_store.index_to_docstore_id)}\")\n",
    "\n",
    "vector_store_retriever = vector_store.as_retriever()\n",
    "print(f\"vector_store_retriever: {vector_store_retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information provided in the context about LangSmith's capabilities or features related to testing. The text only mentions that LangSmith is a project or organization (indicated by the \"LangSmith SDK\" and \"LangChain Python Docs\" links), but it does not provide any details on how it can be used for testing.\n"
     ]
    }
   ],
   "source": [
    "# 5. create a chat prompt template\n",
    "# 6. create a stuff document chain that accepts a llm model and chat prompt template & we can also run stuff document chain by passing in documents directly\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(\n",
    "    llm=llm, prompt=chat_prompt_template)\n",
    "response = documents_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"how can langsmith help with testing?\",\n",
    "        \"context\": documents\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'answer': 'There is no information provided in the context about '\n",
      "              \"LangSmith's capabilities or features related to testing. The \"\n",
      "              'text only mentions that LangSmith is a project or organization '\n",
      "              '(indicated by the \"LangSmith SDK\" and \"LangChain Python Docs\" '\n",
      "              'links), but it does not provide any details on how it can be '\n",
      "              'used for testing.',\n",
      "    'context': [   Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')],\n",
      "    'input': 'how can langsmith help with testing?'}\n"
     ]
    }
   ],
   "source": [
    "# 7. create a document retrieval chain that takes vector store retriever and stuff document chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    vector_store_retriever, documents_chain)\n",
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "# print(type(response))\n",
    "pprint.pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "history_aware_retriever_chain = create_history_aware_retriever(\n",
    "    llm, vector_store_retriever, chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"We'd be happy to help you test your Large Language Model (LLM) \"\n",
      "           'applications. Here are some ways we can assist:\\n'\n",
      "           '\\n'\n",
      "           '1. **Conversational Testing**: We can engage in conversations with '\n",
      "           'your LLM, providing it with a variety of prompts and scenarios to '\n",
      "           'test its understanding, accuracy, and response quality.\\n'\n",
      "           '2. **Error Identification**: Our team can help identify errors or '\n",
      "           \"biases in your LLM's responses, such as incorrect information, \"\n",
      "           'inconsistencies, or inappropriate content.\\n'\n",
      "           '3. **Performance Evaluation**: We can evaluate the performance of '\n",
      "           'your LLM on specific tasks, such as answering questions, '\n",
      "           'generating text, or completing tasks.\\n'\n",
      "           '4. **Data Quality Assessment**: We can assess the quality and '\n",
      "           \"relevance of the data used to train your LLM, ensuring it's \"\n",
      "           'accurate, diverse, and up-to-date.\\n'\n",
      "           '\\n'\n",
      "           'To get started, please provide more details about your LLM '\n",
      "           'application, such as:\\n'\n",
      "           '\\n'\n",
      "           '* What specific tasks or domains does it focus on?\\n'\n",
      "           '* What are your goals for testing and improving the model?\\n'\n",
      "           \"* Do you have any existing test data or scenarios you'd like us to \"\n",
      "           'use?\\n'\n",
      "           '\\n'\n",
      "           \"We'll work with you to create a customized testing plan that meets \"\n",
      "           'your needs.',\n",
      " 'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})],\n",
      " 'context': [Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')],\n",
      " 'input': 'tell me how'}\n"
     ]
    }
   ],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, chat_prompt_template)\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    history_aware_retriever_chain, document_chain)\n",
    "\n",
    "chat_history = [HumanMessage(\n",
    "    content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"tell me how\"\n",
    "})\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_embedding = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")\n",
    "# ollama_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "ollama_embedding = OllamaEmbeddings(model=\"bge-m3:567m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: postgresql+psycopg2://user:password@host:port/dbname\n",
    "# Database Connection Details\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "COLLECTION_NAME = \"dbpedia_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to PGVector 'dbpedia_docs'...\n",
      "connection successfull!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nConnecting to PGVector '{COLLECTION_NAME}'...\")\n",
    "try:\n",
    "    # If the collection table doesn't exist, PGVector will try to create it.\n",
    "    vectorstore = PGVector(\n",
    "        connection=CONNECTION_STRING,\n",
    "        embeddings=ollama_embedding,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        use_jsonb=True\n",
    "        # pre_delete_collection=True\n",
    "        # Use pre_delete_collection=True if you want to clear the collection on every run (USE WITH CAUTION!)\n",
    "        # pre_delete_collection=False,\n",
    "    )\n",
    "    print(f\"connection successfull!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    print(f\"\\nDatabase Connection Error: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during PGVector connection: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to ontotext graph db and fetch all the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHDB_BASE_URL = os.getenv(\"GRAPHDB_BASE_URL\")\n",
    "GRAPHDB_REPOSITORY = os.getenv(\"GRAPHDB_REPOSITORY\")\n",
    "\n",
    "# Format: {base_url}/repositories/{repository_id}\n",
    "SPARQL_ENDPOINT = urljoin(GRAPHDB_BASE_URL.strip('/') + '/', f\"repositories/{GRAPHDB_REPOSITORY}\")\n",
    "MAX_THREADS = os.cpu_count() or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = r\"\"\"\n",
    "# PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "# SELECT ?class\n",
    "# FROM <http://dbpedia.org/model>\n",
    "# WHERE {\n",
    "#   ?class a owl:Class .\n",
    "#   FILTER (\n",
    "#     regex(STRAFTER(STR(?class), \"http://dbpedia.org/ontology/\"), \"^[\\\\x00-\\\\x7F]+$\")\n",
    "#   )\n",
    "# }\n",
    "# \"\"\"\n",
    "# sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "# sparql.setReturnFormat(JSON)\n",
    "\n",
    "# entitiies = []\n",
    "\n",
    "# try:\n",
    "#     sparql.setQuery(query)\n",
    "#     results = sparql.query().convert()\n",
    "#     for result in results[\"results\"][\"bindings\"]:\n",
    "#         entitiies.append(result[\"class\"][\"value\"])\n",
    "#         # print(result[\"class\"][\"value\"])\n",
    "# except Exception as e:\n",
    "#     print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639,\n",
       " ['http://dbpedia.org/ontology/AcademicConference',\n",
       "  'http://dbpedia.org/ontology/AcademicJournal'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(entitiies), entitiies[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sparql():\n",
    "#     sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "#     sparql.setReturnFormat(JSON)\n",
    "#     return sparql\n",
    "\n",
    "# # Step 3: Describe a single instance\n",
    "# def describe_instance(instance_iri):\n",
    "#     sparql = get_sparql()\n",
    "#     sparql.setReturnFormat(N3)\n",
    "#     sparql.setQuery(f\"DESCRIBE <{instance_iri}>\")\n",
    "#     try:\n",
    "#         rdf = sparql.query().convert()\n",
    "#         rdf_str = rdf.decode('utf-8') if isinstance(rdf, bytes) else str(rdf)\n",
    "#         return rdf_str\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Error] Describing {instance_iri}: {e}\")\n",
    "#         try:\n",
    "#             folder_location = os.path.join(os.getcwd(), \"datasets\", \"failed\")\n",
    "#             os.makedirs(folder_location, exist_ok=True)  # ensure 'data/' exists\n",
    "#             failed_file_with_iri = os.path.join(folder_location, \"failed_instances_iri.txt\")\n",
    "#             with open(failed_file_with_iri, \"a\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(instance_iri + \"\\n\")\n",
    "#             print(f\"Logged failed IRI to {failed_file_with_iri}\")\n",
    "#         except Exception as file_err:\n",
    "#             print(f\"[Error] Saving failed IRI: {file_err}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# url = \"http://dbpedia.org/resource/%22V%22_Is_for_Vengeance\"\n",
    "# str_format_rdf = describe_instance(instance_iri=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME_DIR = os.path.join(\n",
    "    \"c:\\\\Users\\\\deepa\\\\data\\\\workspace\\\\notebooks\", \"datasets\", \"instance_description\")\n",
    "OUTPUT_FILENAME = os.path.join(\n",
    "    OUTPUT_FILENAME_DIR, \"instance_description.jsonl\")\n",
    "\n",
    "FAILED_LOG_DIR = os.path.join(\n",
    "    \"c:\\\\Users\\\\deepa\\\\data\\\\workspace\\\\notebooks\", \"datasets\", \"failed\")\n",
    "FAILED_CLASS_LOG = os.path.join(FAILED_LOG_DIR, \"failed_class_iri.txt\")\n",
    "FAILED_INSTANCE_LOG = os.path.join(FAILED_LOG_DIR, \"failed_instance_iri.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ontology classes...\n",
      "Fetched 10 sample classes.\n",
      "Processing 10 classes using up to 32 threads...\n",
      " > Progress: 10/10 classes processed... \n",
      "Processing complete. Descriptions saved to c:\\Users\\deepa\\data\\workspace\\notebooks\\datasets\\instance_description\\instance_description.jsonl\n",
      "Check c:\\Users\\deepa\\data\\workspace\\notebooks\\datasets\\failed\\failed_class_iri.txt and c:\\Users\\deepa\\data\\workspace\\notebooks\\datasets\\failed\\failed_instance_iri.txt for any errors.\n"
     ]
    }
   ],
   "source": [
    "# --- Global Lock for File Writing ---\n",
    "# This single lock will protect all file write operations (main output + error logs)\n",
    "file_lock = Lock()\n",
    "\n",
    "# --- SPARQL and Helper Functions ---\n",
    "\n",
    "\n",
    "def get_sparql(return_format=JSON):\n",
    "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "    sparql.setReturnFormat(return_format)\n",
    "    return sparql\n",
    "\n",
    "\n",
    "def split_camel_case_to_lower_words(name):\n",
    "    \"\"\"Splits CamelCase or PascalCase and returns lowercase words separated by spaces.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    # Handle simple cases first\n",
    "    if name.islower() or '_' in name or not re.search('[A-Z]', name):\n",
    "        # Replace underscores and lowercase\n",
    "        return name.replace('_', ' ').lower()\n",
    "\n",
    "    # Insert space before uppercase letters (except at the start)\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', name)\n",
    "    # Insert space before uppercase letters that follow lowercase or digit\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', s1)\n",
    "    return s2.lower()  # Convert the whole result to lowercase\n",
    "\n",
    "\n",
    "def clean_uri_for_llm_key(uri_str):\n",
    "    \"\"\"Cleans a predicate URI string into a readable key (lowercase, space-separated).\"\"\"\n",
    "    if not uri_str:\n",
    "        return \"unknown property\"\n",
    "\n",
    "    # Specific overrides first (already lowercase)\n",
    "    if uri_str == str(RDF.type):\n",
    "        return \"type\"\n",
    "    if uri_str == str(RDFS.label):\n",
    "        return \"label\"\n",
    "    if uri_str == str(FOAF.name):\n",
    "        return \"name\"\n",
    "    # Add other specific overrides if needed (e.g., DCTERMS.subject -> \"subject\")\n",
    "\n",
    "    # General cleaning - extract local name\n",
    "    if '#' in uri_str:\n",
    "        name = uri_str.split('#')[-1]\n",
    "    else:\n",
    "        name = uri_str.split('/')[-1]\n",
    "\n",
    "    # Split camel case and convert to lowercase words\n",
    "    return split_camel_case_to_lower_words(name)\n",
    "\n",
    "\n",
    "def clean_uri_for_llm_value(uri_str):\n",
    "    \"\"\"Cleans a resource URI string into a readable value for LLM output.\"\"\"\n",
    "    if not uri_str:\n",
    "        return \"Unknown Resource\"\n",
    "    if '#' in uri_str:\n",
    "        name = uri_str.split('#')[-1]\n",
    "    else:\n",
    "        name = uri_str.split('/')[-1]\n",
    "    # Basic URL decoding for parentheses\n",
    "    name = name.replace('%28', '(').replace('%29', ')')\n",
    "    # Special handling for Wikidata URIs to just show the QID\n",
    "    if uri_str.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "        return name  # Just return QID like Q215380\n",
    "    # Default: replace underscores with spaces\n",
    "    return name.replace('_', ' ')\n",
    "\n",
    "\n",
    "def format_rdf_term_for_llm_value(term_data):\n",
    "    \"\"\"\n",
    "    Formats a term (represented as dict from Step 1 or URI string)\n",
    "    into a simple string value for LLM output.\n",
    "    \"\"\"\n",
    "    if isinstance(term_data, dict):  # Literal dictionary\n",
    "        val = term_data.get(\"value\", \"\")\n",
    "        # Clean common literal suffixes for LLM readability\n",
    "        val = re.sub(r'@\\w+(-[A-Za-z0-9]+)*$', '', val)  # Remove @lang tags\n",
    "        val = re.sub(r'\\^\\^<.*>$', '', val)  # Remove ^^<datatype>\n",
    "        val = val.strip('\"')  # Remove surrounding quotes if any\n",
    "        return val\n",
    "    elif isinstance(term_data, str):  # URI string\n",
    "        return clean_uri_for_llm_value(term_data)\n",
    "    else:\n",
    "        return str(term_data)  # Fallback\n",
    "\n",
    "\n",
    "def format_rdf_term(term):\n",
    "    \"\"\"Creates the intermediate structured representation for RDF terms.\"\"\"\n",
    "    if isinstance(term, Literal):\n",
    "        dt = str(term.datatype) if term.datatype else None\n",
    "        # Assign default datatypes if missing\n",
    "        if dt is None and term.language:\n",
    "            dt = str(RDF.langString)\n",
    "        elif dt is None:\n",
    "            dt = str(XSD.string)\n",
    "        return {\"value\": str(term), \"language\": term.language, \"datatype\": dt}\n",
    "    elif isinstance(term, URIRef):\n",
    "        return str(term)\n",
    "    else:  # Handle Blank Nodes etc.\n",
    "        return str(term)\n",
    "\n",
    "\n",
    "def extract_structured_description(rdf_n3_string, instance_iri):\n",
    "    \"\"\"Parses N3 RDF data and extracts outgoing/incoming relationships.\"\"\"\n",
    "    if not rdf_n3_string:\n",
    "        # Return empty structure if no N3 data provided (e.g., empty DESCRIBE/CONSTRUCT)\n",
    "        return {\"instance_iri\": instance_iri, \"outgoing\": {}, \"incoming\": {}}\n",
    "    g = Graph()\n",
    "    try:\n",
    "        # Use instance_iri as base URI for resolving relative URIs if any\n",
    "        g.parse(data=rdf_n3_string, format=\"n3\", publicID=instance_iri)\n",
    "    except Exception as e:\n",
    "        # Log parsing errors specifically\n",
    "        print(\n",
    "            f\"[Error] Parsing N3 data for {instance_iri}: {type(e).__name__} - {e}\")\n",
    "        return None  # Indicate failure\n",
    "    instance_ref = URIRef(instance_iri)\n",
    "    outgoing_data = defaultdict(list)\n",
    "    incoming_data = defaultdict(list)\n",
    "\n",
    "    # Outgoing properties\n",
    "    for pred, obj in g.predicate_objects(subject=instance_ref):\n",
    "        pred_uri_str = str(pred)\n",
    "        formatted_obj = format_rdf_term(obj)\n",
    "        # Avoid adding exact duplicates (important for literals)\n",
    "        if formatted_obj not in outgoing_data[pred_uri_str]:\n",
    "            outgoing_data[pred_uri_str].append(formatted_obj)\n",
    "\n",
    "    # Incoming relationships\n",
    "    for subj, pred in g.subject_predicates(object=instance_ref):\n",
    "        # Avoid reflexive triples (where subject is the instance itself)\n",
    "        if subj == instance_ref:\n",
    "            continue\n",
    "        pred_uri_str = str(pred)\n",
    "        subj_uri_str = str(subj)\n",
    "        # Avoid adding duplicate incoming subjects for the same predicate\n",
    "        if subj_uri_str not in incoming_data[pred_uri_str]:\n",
    "            incoming_data[pred_uri_str].append(subj_uri_str)\n",
    "\n",
    "    # Final Structure: Convert defaultdicts, sort values for consistency\n",
    "    final_outgoing = {pred: sorted(values, key=str)\n",
    "                      for pred, values in outgoing_data.items()}\n",
    "    # Sort incoming subjects as well\n",
    "    final_incoming = {pred: sorted(values)\n",
    "                      for pred, values in incoming_data.items()}\n",
    "    return {\"instance_iri\": instance_iri, \"outgoing\": final_outgoing, \"incoming\": final_incoming}\n",
    "\n",
    "\n",
    "def format_for_llm_custom_layout(structured_data):\n",
    "    \"\"\"\n",
    "    Takes the structured dictionary and formats it into the specific\n",
    "    two-part layout requested by the user (revised key/predicate format).\n",
    "    \"\"\"\n",
    "    if not structured_data or (not structured_data.get(\"outgoing\") and not structured_data.get(\"incoming\")):\n",
    "        instance_iri = structured_data.get(\"instance_iri\", \"Unknown Instance\")\n",
    "        instance_name = clean_uri_for_llm_value(instance_iri)\n",
    "        # Provide a minimal output even if no data found after parsing\n",
    "        return f\"name: {instance_name}\\n(No description properties found)\"\n",
    "\n",
    "    instance_iri = structured_data.get(\"instance_iri\")\n",
    "    instance_name_cleaned = clean_uri_for_llm_value(instance_iri)\n",
    "\n",
    "    output_lines_part1 = []\n",
    "    output_lines_part2 = []\n",
    "\n",
    "    # --- Part 1: Outgoing Properties (key: value) ---\n",
    "    outgoing_properties = structured_data.get(\"outgoing\", {})\n",
    "    primary_name_val = instance_name_cleaned  # Default name\n",
    "\n",
    "    temp_outgoing_formatted = {}\n",
    "    for pred_uri in sorted(outgoing_properties.keys()):\n",
    "        llm_key = clean_uri_for_llm_key(pred_uri)\n",
    "        values = outgoing_properties[pred_uri]\n",
    "        cleaned_values_for_key = []\n",
    "        for term_data in values:\n",
    "            cleaned_val = format_rdf_term_for_llm_value(term_data)\n",
    "            # Add value if it's not empty and not already added\n",
    "            if cleaned_val and cleaned_val not in cleaned_values_for_key:\n",
    "                cleaned_values_for_key.append(cleaned_val)\n",
    "        if cleaned_values_for_key:\n",
    "            # Sort the cleaned values before joining\n",
    "            value_string = \", \".join(sorted(cleaned_values_for_key))\n",
    "            temp_outgoing_formatted[llm_key] = value_string\n",
    "            # Update primary name if this is the 'name' key\n",
    "            if llm_key == 'name':\n",
    "                primary_name_val = value_string\n",
    "\n",
    "    # Generate output lines for part 1, ensuring 'name' is first\n",
    "    if 'name' in temp_outgoing_formatted:\n",
    "        output_lines_part1.append(f\"name: {temp_outgoing_formatted['name']}\")\n",
    "    elif instance_name_cleaned:  # Add fallback if no name property found\n",
    "        output_lines_part1.append(f\"name: {instance_name_cleaned}\")\n",
    "\n",
    "    # Add other properties sorted by key\n",
    "    for key in sorted(temp_outgoing_formatted.keys()):\n",
    "        if key == 'name':\n",
    "            continue  # Skip name as it's already added\n",
    "        output_lines_part1.append(f\"{key}: {temp_outgoing_formatted[key]}\")\n",
    "\n",
    "    # --- Part 2: Incoming Relationships (Subject : Predicate : Object) ---\n",
    "    incoming_relationships = structured_data.get(\"incoming\", {})\n",
    "    instance_name_for_part2 = primary_name_val  # Use name determined in Part 1\n",
    "\n",
    "    incoming_tuples = []\n",
    "    for pred_uri, subjects in incoming_relationships.items():\n",
    "        # Get cleaned predicate name (lowercase, space-separated)\n",
    "        if '#' in pred_uri:\n",
    "            pred_local_name = pred_uri.split('#')[-1]\n",
    "        else:\n",
    "            pred_local_name = pred_uri.split('/')[-1]\n",
    "        pred_cleaned_for_output = split_camel_case_to_lower_words(\n",
    "            pred_local_name)\n",
    "\n",
    "        # Create a separate entry for each subject\n",
    "        for subj_uri in subjects:\n",
    "            cleaned_subj = clean_uri_for_llm_value(subj_uri)\n",
    "            if cleaned_subj:\n",
    "                # Add tuple: (cleaned_subject, cleaned_predicate, instance_name)\n",
    "                incoming_tuples.append(\n",
    "                    (cleaned_subj, pred_cleaned_for_output, instance_name_for_part2))\n",
    "\n",
    "    # Sort the tuples primarily by subject name, then by predicate name\n",
    "    incoming_tuples.sort()\n",
    "\n",
    "    # Generate output lines for part 2 from sorted tuples\n",
    "    for subj, pred, obj in incoming_tuples:\n",
    "        output_lines_part2.append(f\"{subj} : {pred} : {obj}\")\n",
    "\n",
    "    # --- Combine Output ---\n",
    "    final_output = \"\\n\".join(output_lines_part1)\n",
    "    # Add separator only if both parts have content\n",
    "    if output_lines_part1 and output_lines_part2:\n",
    "        final_output += \"\\n\\n\"  # Add blank line separator\n",
    "    if output_lines_part2:\n",
    "        final_output += \"\\n\".join(output_lines_part2)\n",
    "\n",
    "    # Handle cases where only incoming relationships were found\n",
    "    if not output_lines_part1 and output_lines_part2:\n",
    "        final_output = f\"name: {instance_name_cleaned}\\n(No outgoing properties found)\\n\\n\" + \\\n",
    "            final_output\n",
    "\n",
    "    return final_output\n",
    "\n",
    "# --- Core Logic Functions ---\n",
    "\n",
    "# Step 1: Fetch all ontology classes\n",
    "\n",
    "\n",
    "def fetch_classes():\n",
    "    \"\"\"Fetches a sample of DBpedia ontology classes.\"\"\"\n",
    "    print(\"Fetching ontology classes...\")\n",
    "    sparql = get_sparql(return_format=JSON)\n",
    "    # Refined query for DBpedia ontology classes\n",
    "    class_query = r\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "    SELECT ?class\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    WHERE {\n",
    "      ?class a owl:Class .\n",
    "      FILTER (\n",
    "        regex(STRAFTER(STR(?class), \"http://dbpedia.org/ontology/\"), \"^[\\\\x00-\\\\x7F]+$\")\n",
    "      )\n",
    "    }\n",
    "    ORDER BY ?class\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    sparql.setQuery(class_query)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        classes = [result[\"class\"][\"value\"]\n",
    "                   for result in results[\"results\"][\"bindings\"]]\n",
    "        print(f\"Fetched {len(classes)} sample classes.\")\n",
    "        return classes\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Fetching classes: {type(e).__name__} - {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Step 2: Fetch instances of a class\n",
    "def fetch_instances_for_class(ontology_class):\n",
    "    \"\"\"Fetches a sample of instances for a given DBpedia class.\"\"\"\n",
    "    sparql = get_sparql(return_format=JSON)\n",
    "    instance_query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?instance\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    FROM <http://dbpedia.org/data>\n",
    "    WHERE {{\n",
    "        BIND(<{ontology_class}> AS ?entity)\n",
    "        ?instance a ?entity .\n",
    "    }}\n",
    "    ORDER BY ?instance\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(instance_query)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        instances = [result[\"instance\"][\"value\"]\n",
    "                     for result in results[\"results\"][\"bindings\"]]\n",
    "        return instances\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[Error] Fetching instances for {ontology_class}: {type(e).__name__} - {e}\")\n",
    "        # Log failed class with thread safety\n",
    "        try:\n",
    "            os.makedirs(FAILED_LOG_DIR, exist_ok=True)\n",
    "            with file_lock:  # Protect file write\n",
    "                with open(FAILED_CLASS_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(ontology_class + \"\\n\")\n",
    "        except Exception as file_err:\n",
    "            print(\n",
    "                f\"[Error] Saving failed class IRI to {FAILED_CLASS_LOG}: {file_err}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Step 3: Describe a single instance\n",
    "def describe_instance(instance_iri):\n",
    "    \"\"\"\n",
    "    Fetches description data (N3), processes it structurally, and formats for LLM.\n",
    "    Uses CONSTRUCT for potentially better reliability than DESCRIBE.\n",
    "    \"\"\"\n",
    "    sparql = get_sparql(return_format=N3)\n",
    "    query = f\"DESCRIBE <{instance_iri}>\"\n",
    "    sparql.setQuery(query)\n",
    "    try:\n",
    "        result_bytes = sparql.query().convert()\n",
    "        rdf_n3_string = result_bytes.decode('utf-8') if result_bytes else \"\"\n",
    "\n",
    "        # Step 1 (Internal): Extract structured data\n",
    "        structured_data = extract_structured_description(\n",
    "            rdf_n3_string, instance_iri)\n",
    "\n",
    "        if structured_data is None:\n",
    "            return None  # Error during parsing\n",
    "\n",
    "        # Step 2 (Internal): Format for LLM\n",
    "        llm_input_string = format_for_llm_custom_layout(structured_data)\n",
    "        return llm_input_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Describing {instance_iri}: {type(e).__name__} - {e}\")\n",
    "        # Log failed instance with thread safety\n",
    "        try:\n",
    "            os.makedirs(FAILED_LOG_DIR, exist_ok=True)\n",
    "            with file_lock:  # Protect file write\n",
    "                with open(FAILED_INSTANCE_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(instance_iri + \"\\n\")\n",
    "        except Exception as file_err:\n",
    "            print(\n",
    "                f\"[Error] Saving failed instance IRI to {FAILED_INSTANCE_LOG}: {file_err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Step 4: Threaded orchestration\n",
    "def process_class(ontology_class, output_filename, lock):\n",
    "    \"\"\"Fetches instances for a class and writes their descriptions to a file.\"\"\"\n",
    "    instances = fetch_instances_for_class(ontology_class)\n",
    "    if not instances:\n",
    "        return\n",
    "\n",
    "    for iri in instances:\n",
    "        describe_instance_str = describe_instance(iri)\n",
    "        if describe_instance_str is not None:\n",
    "           # Create a dictionary for the JSON object\n",
    "            output_data = {\n",
    "                \"iri\": iri,\n",
    "                \"description\": describe_instance_str\n",
    "            }\n",
    "            # Convert dictionary to JSON string\n",
    "            # ensure_ascii=False is important for non-ASCII characters in descriptions/IRIs\n",
    "            json_line = json.dumps(output_data, ensure_ascii=False)\n",
    "\n",
    "            # Acquire lock before writing to the shared file\n",
    "            with lock:\n",
    "                try:\n",
    "                    with open(output_filename, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(json_line + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[Error] Writing to file for {iri}: {e}\")\n",
    "\n",
    "\n",
    "# Step 5: Main runner with threading\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate fetching and processing.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(FAILED_LOG_DIR, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Creating failed log directory {FAILED_LOG_DIR}: {e}\")\n",
    "        return\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_FILENAME_DIR, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Creating failed log directory {FAILED_LOG_DIR}: {e}\")\n",
    "        return\n",
    "\n",
    "    owl_classes = fetch_classes()\n",
    "    if not owl_classes:\n",
    "        print(\"No classes fetched. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\n",
    "        f\"Processing {len(owl_classes)} classes using up to {MAX_THREADS} threads...\")\n",
    "\n",
    "    processed_count = 0\n",
    "    # Use the global lock defined earlier\n",
    "    global file_lock\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        # Submit tasks with necessary arguments\n",
    "        futures = {executor.submit(process_class, owl_class, OUTPUT_FILENAME, file_lock): owl_class\n",
    "                   for owl_class in owl_classes}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            owl_class = futures[future]\n",
    "            try:\n",
    "                future.result()  # Check for exceptions raised within the thread\n",
    "                processed_count += 1\n",
    "                # Print progress indicator\n",
    "                print(\n",
    "                    f\" \\r > Progress: {processed_count}/{len(owl_classes)} classes processed...\", end=\"\")\n",
    "            except Exception as e:\n",
    "                # Log errors from the thread execution itself\n",
    "                print(\n",
    "                    f\"\\n[Error in thread result for {owl_class}]: {type(e).__name__} - {e}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete. Descriptions saved to {OUTPUT_FILENAME}\")\n",
    "    print(f\"Check {FAILED_CLASS_LOG} and {FAILED_INSTANCE_LOG} for any errors.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/100_Word_Story\n",
      "Description: name: 100 Word Story\n",
      "abbreviation: 100 Word Story\n",
      "academic discipline: Literary magazine\n",
      "editor: Grant Faulkner\n",
      "first publication year: 2011\n",
      "frequency of publication: Quarterly\n",
      "type: AcademicJournal, CreativeWork, PeriodicalLiterature, Q1092563, Q234460, Q386724, Thing, Work, WrittenWork\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/AAAI/ACM_Conference_on_AI,_Ethics,_and_Society\n",
      "Description: name: AIES\n",
      "academic discipline: Computer science\n",
      "frequency of publication: Annual\n",
      "publisher: Association for Computing Machinery\n",
      "type: AcademicConference, Event, Q1656682, Q2020153, SocietalEvent, Thing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/A.J._Applegate\n",
      "Description: name: A.J. Applegate\n",
      "birth place: Massapequa, New York (state)\n",
      "type: Actor, AdultActor, Animal, Artist, Eukaryote, NaturalPerson, Person, Q19088, Q215627, Q33999, Q483501, Q488111, Q5, Q729, Species, Thing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/%22Glozel_est_Authentique!%22\n",
      "Description: name: Glozel est Authentique!\n",
      "genre: Horror fiction\n",
      "publisher: Theatre of the Mind Enterprises\n",
      "type: Activity, Game, Q11410, Q1914636, Thing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/'Arsh\n",
      "Description: name:  øArsh\n",
      "country: Yemen\n",
      "elevation: 1455.0\n",
      "original name: ÿπÿ±ÿ¥\n",
      "population total: 9569\n",
      "subdivision: Al-Misrakh District, Taiz Governorate\n",
      "type: AdministrativeArea, AdministrativeRegion, Location, Place, PopulatedPlace, Q3455524, Region, Thing\n",
      "utc offset: +3\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/A.J._Applegate\n",
      "Description: name: A.J. Applegate\n",
      "birth place: Massapequa, New York (state)\n",
      "type: Actor, AdultActor, Animal, Artist, Eukaryote, NaturalPerson, Person, Q19088, Q215627, Q33999, Q483501, Q488111, Q5, Q729, Species, Thing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/14-X\n",
      "Description: name: 14-X\n",
      "manufacturer: Department of Aerospace Science and Technology\n",
      "origin: Brazil\n",
      "see also: 14-XS\n",
      "type: Aircraft, MeanOfTransportation, Product, Q11436, Thing\n",
      "\n",
      "14-XS : spacecraft : 14-X\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IRI: http://dbpedia.org/resource/!!!\n",
      "Description: name: !!!\n",
      "active years start year: 1996\n",
      "alias: Chk Chk Chk\n",
      "background: group_or_band\n",
      "band member: Nic Offer\n",
      "former band member: Jerry Fuchs, Justin Van Der Volgen\n",
      "genre: Alternative dance, Dance-punk, Disco-rock, Funk rock, Indietronica\n",
      "hometown: California, Sacramento, California\n",
      "record label: Gold Standard Laboratories, Touch and Go Records, Warp (record label)\n",
      "type: Agent, Band, Group, MusicGroup, Organisation, Organization, Q215380, Q24229398, Q43229, SocialPerson, Thing\n",
      "\n",
      " One Boy : artist : !!!\n",
      "Chris Coady : associated band : !!!\n",
      "Chris Coady : associated musical artist : !!!\n",
      "Jerry Fuchs : associated band : !!!\n",
      "Jerry Fuchs : associated musical artist : !!!\n",
      "Maserati (band) : associated band : !!!\n",
      "Maserati (band) : associated musical artist : !!!\n",
      "Nic Offer  Nic Offer  1 : associated band : !!!\n",
      "Nic Offer  Nic Offer  1 : associated musical artist : !!!\n",
      "Raleigh Moncrief : associated band : !!!\n",
      "Raleigh Moncrief : associated musical artist : !!!\n",
      "The Juan MacLean : associated band : !!!\n",
      "The Juan MacLean : associated musical artist : !!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_one_jsonl(filename):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file line by line and yields each parsed JSON object.\n",
    "    This allows processing one record at a time without loading the whole file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_number, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    yield json.loads(line) # Yield the parsed dictionary\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON on line {line_number} in {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filename}: {e}\")\n",
    "\n",
    "# Example Usage:\n",
    "for record in read_one_jsonl(OUTPUT_FILENAME):\n",
    "    print(\"---\"*50)\n",
    "    print(f\"IRI: {record.get('iri')}\")\n",
    "    print(f\"Description: {record.get('description')}\")\n",
    "    # Process the record here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total owl classes fetched: 10\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/AAAI/ACM_Conference_on_AI,_Ethics,_and_Society\n",
      "name: AIES\n",
      "academic discipline: Computer science\n",
      "frequency of publication: Annual\n",
      "publisher: Association for Computing Machinery\n",
      "type: AcademicConference, Event, Q1656682, Q2020153, SocietalEvent, Thing\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/%22Glozel_est_Authentique!%22\n",
      "name: Glozel est Authentique!\n",
      "genre: Horror fiction\n",
      "publisher: Theatre of the Mind Enterprises\n",
      "type: Activity, Game, Q11410, Q1914636, Thing\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/100_Word_Story\n",
      "name: 100 Word Story\n",
      "abbreviation: 100 Word Story\n",
      "academic discipline: Literary magazine\n",
      "editor: Grant Faulkner\n",
      "first publication year: 2011\n",
      "frequency of publication: Quarterly\n",
      "type: AcademicJournal, CreativeWork, PeriodicalLiterature, Q1092563, Q234460, Q386724, Thing, Work, WrittenWork\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/'Arsh\n",
      "name:  øArsh\n",
      "country: Yemen\n",
      "elevation: 1455.0\n",
      "original name: ÿπÿ±ÿ¥\n",
      "population total: 9569\n",
      "subdivision: Al-Misrakh District, Taiz Governorate\n",
      "type: AdministrativeArea, AdministrativeRegion, Location, Place, PopulatedPlace, Q3455524, Region, Thing\n",
      "utc offset: +3\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/A.J._Applegate\n",
      "name: A.J. Applegate\n",
      "birth place: Massapequa, New York (state)\n",
      "type: Actor, AdultActor, Animal, Artist, Eukaryote, NaturalPerson, Person, Q19088, Q215627, Q33999, Q483501, Q488111, Q5, Q729, Species, Thing\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/A.J._Applegate\n",
      "name: A.J. Applegate\n",
      "birth place: Massapequa, New York (state)\n",
      "type: Actor, AdultActor, Animal, Artist, Eukaryote, NaturalPerson, Person, Q19088, Q215627, Q33999, Q483501, Q488111, Q5, Q729, Species, Thing\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/!!!\n",
      "name: !!!\n",
      "active years start year: 1996\n",
      "alias: Chk Chk Chk\n",
      "background: group_or_band\n",
      "band member: Nic Offer\n",
      "former band member: Jerry Fuchs, Justin Van Der Volgen\n",
      "genre: Alternative dance, Dance-punk, Disco-rock, Funk rock, Indietronica\n",
      "hometown: California, Sacramento, California\n",
      "record label: Gold Standard Laboratories, Touch and Go Records, Warp (record label)\n",
      "type: Agent, Band, Group, MusicGroup, Organisation, Organization, Q215380, Q24229398, Q43229, SocialPerson, Thing\n",
      "\n",
      " One Boy : artist : !!!\n",
      "Chris Coady : associated band : !!!\n",
      "Chris Coady : associated musical artist : !!!\n",
      "Jerry Fuchs : associated band : !!!\n",
      "Jerry Fuchs : associated musical artist : !!!\n",
      "Maserati (band) : associated band : !!!\n",
      "Maserati (band) : associated musical artist : !!!\n",
      "Nic Offer  Nic Offer  1 : associated band : !!!\n",
      "Nic Offer  Nic Offer  1 : associated musical artist : !!!\n",
      "Raleigh Moncrief : associated band : !!!\n",
      "Raleigh Moncrief : associated musical artist : !!!\n",
      "The Juan MacLean : associated band : !!!\n",
      "The Juan MacLean : associated musical artist : !!!\n",
      "\n",
      "\n",
      "[DESCRIBE] http://dbpedia.org/resource/14-X\n",
      "name: 14-X\n",
      "manufacturer: Department of Aerospace Science and Technology\n",
      "origin: Brazil\n",
      "see also: 14-XS\n",
      "type: Aircraft, MeanOfTransportation, Product, Q11436, Thing\n",
      "\n",
      "14-XS : spacecraft : 14-X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sparql():\n",
    "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql\n",
    "\n",
    "# Step 1: Fetch all ontology classes\n",
    "def fetch_classes():\n",
    "    sparql = get_sparql()\n",
    "    class_query = r\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "    SELECT ?class\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    WHERE {\n",
    "      ?class a owl:Class .\n",
    "      FILTER (\n",
    "        regex(STRAFTER(STR(?class), \"http://dbpedia.org/ontology/\"), \"^[\\\\x00-\\\\x7F]+$\")\n",
    "      )\n",
    "    }\n",
    "    ORDER BY ?class\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    sparql.setQuery(class_query)\n",
    "    results = sparql.query().convert()\n",
    "    return [result[\"class\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
    "\n",
    "# Step 2: Fetch instances of a class\n",
    "def fetch_instances_for_class(ontology_class):\n",
    "    sparql = get_sparql()\n",
    "    instance_query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?instance\n",
    "    FROM <http://dbpedia.org/model>\n",
    "    FROM <http://dbpedia.org/data>\n",
    "    WHERE {{\n",
    "        BIND(<{ontology_class}> AS ?entity)\n",
    "        ?instance a ?entity .\n",
    "    }}\n",
    "    ORDER BY ?instance\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(instance_query)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        return [(result[\"instance\"][\"value\"]) for result in results[\"results\"][\"bindings\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Fetching instances for {ontology_class}: {e}\")\n",
    "        try:\n",
    "            folder_location = os.path.join(os.getcwd(), \"datasets\", \"failed\")\n",
    "            os.makedirs(folder_location, exist_ok=True)\n",
    "            failed_class_file = os.path.join(\n",
    "                folder_location, \"failed_class_iri.txt\")\n",
    "            with open(failed_class_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(ontology_class + \"\\n\")\n",
    "            print(f\"Logged failed class IRI to {failed_class_file}\")\n",
    "        except Exception as file_err:\n",
    "            print(f\"[Error] Saving failed class IRI: {file_err}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# def get_sparql():\n",
    "#     sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "#     return sparql\n",
    "\n",
    "\n",
    "def split_camel_case_to_lower_words(name):\n",
    "    \"\"\"Splits CamelCase or PascalCase and returns lowercase words separated by spaces.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    # Handle simple cases first\n",
    "    if name.islower() or '_' in name or not re.search('[A-Z]', name):\n",
    "        # Replace underscores and lowercase\n",
    "        return name.replace('_', ' ').lower()\n",
    "\n",
    "    # Insert space before uppercase letters (except at the start)\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', name)\n",
    "    # Insert space before uppercase letters that follow lowercase or digit\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', s1)\n",
    "    return s2.lower()  # Convert the whole result to lowercase\n",
    "\n",
    "\n",
    "def clean_uri_for_llm_key(uri_str):\n",
    "    \"\"\"Cleans a predicate URI string into a readable key (lowercase, space-separated).\"\"\"\n",
    "    if not uri_str:\n",
    "        return \"unknown property\"\n",
    "\n",
    "    # Specific overrides first (already lowercase)\n",
    "    if uri_str == str(RDF.type):\n",
    "        return \"type\"\n",
    "    if uri_str == str(RDFS.label):\n",
    "        return \"label\"\n",
    "    if uri_str == str(FOAF.name):\n",
    "        return \"name\"\n",
    "    # Add other specific overrides if needed\n",
    "\n",
    "    # General cleaning - extract local name\n",
    "    if '#' in uri_str:\n",
    "        name = uri_str.split('#')[-1]\n",
    "    else:\n",
    "        name = uri_str.split('/')[-1]\n",
    "\n",
    "    # Split camel case and convert to lowercase words\n",
    "    return split_camel_case_to_lower_words(name)\n",
    "\n",
    "\n",
    "def clean_uri_for_llm_value(uri_str):\n",
    "    if not uri_str:\n",
    "        return \"Unknown Resource\"\n",
    "    if '#' in uri_str:\n",
    "        name = uri_str.split('#')[-1]\n",
    "    else:\n",
    "        name = uri_str.split('/')[-1]\n",
    "    name = name.replace('%28', '(').replace('%29', ')')\n",
    "    if uri_str.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "        return name  # Just return QID like Q215380\n",
    "    return name.replace('_', ' ')\n",
    "\n",
    "\n",
    "def format_rdf_term_for_llm_value(term_data):\n",
    "    if isinstance(term_data, dict):  # Literal dictionary\n",
    "        val = term_data.get(\"value\", \"\")\n",
    "        val = re.sub(r'@\\w+$', '', val)\n",
    "        val = re.sub(r'\\^\\^<.*>$', '', val)\n",
    "        val = val.strip('\"')\n",
    "        return val\n",
    "    elif isinstance(term_data, str):  # URI string\n",
    "        return clean_uri_for_llm_value(term_data)\n",
    "    else:\n",
    "        return str(term_data)\n",
    "\n",
    "\n",
    "def format_rdf_term(term):\n",
    "    # ... (same as before) ...\n",
    "    if isinstance(term, Literal):\n",
    "        dt = str(term.datatype) if term.datatype else None\n",
    "        if dt is None and term.language:\n",
    "            dt = str(RDF.langString)\n",
    "        elif dt is None:\n",
    "            dt = str(XSD.string)\n",
    "        return {\"value\": str(term), \"language\": term.language, \"datatype\": dt}\n",
    "    elif isinstance(term, URIRef):\n",
    "        return str(term)\n",
    "    else:\n",
    "        return str(term)\n",
    "\n",
    "\n",
    "def extract_structured_description(rdf_n3_string, instance_iri):\n",
    "    # ... (same as before) ...\n",
    "    if not rdf_n3_string:\n",
    "        return {\"instance_iri\": instance_iri, \"outgoing\": {}, \"incoming\": {}}\n",
    "    g = Graph()\n",
    "    try:\n",
    "        g.parse(data=rdf_n3_string, format=\"n3\", publicID=instance_iri)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Parsing N3 data for {instance_iri}: {e}\")\n",
    "        return None\n",
    "    instance_ref = URIRef(instance_iri)\n",
    "    outgoing_data = defaultdict(list)\n",
    "    incoming_data = defaultdict(list)\n",
    "    # Outgoing\n",
    "    for pred, obj in g.predicate_objects(subject=instance_ref):\n",
    "        pred_uri_str = str(pred)\n",
    "        formatted_obj = format_rdf_term(obj)\n",
    "        if formatted_obj not in outgoing_data[pred_uri_str]:\n",
    "            outgoing_data[pred_uri_str].append(formatted_obj)\n",
    "    # Incoming\n",
    "    for subj, pred in g.subject_predicates(object=instance_ref):\n",
    "        if subj == instance_ref:\n",
    "            continue\n",
    "        pred_uri_str = str(pred)\n",
    "        subj_uri_str = str(subj)\n",
    "        incoming_data[pred_uri_str].append(subj_uri_str)\n",
    "    # Final Structure\n",
    "    final_outgoing = {pred: sorted(values, key=str)\n",
    "                      for pred, values in outgoing_data.items()}\n",
    "    final_incoming = {pred: values for pred, values in incoming_data.items()}\n",
    "    return {\"instance_iri\": instance_iri, \"outgoing\": final_outgoing, \"incoming\": final_incoming}\n",
    "\n",
    "\n",
    "def format_for_llm_custom_layout(structured_data):\n",
    "    \"\"\"\n",
    "    Takes the structured dictionary and formats it into the specific\n",
    "    two-part layout requested by the user (revised key/predicate format).\n",
    "\n",
    "    Args:\n",
    "        structured_data: The dictionary produced by extract_structured_description.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string for LLM input in the custom layout.\n",
    "    \"\"\"\n",
    "    if not structured_data or (not structured_data.get(\"outgoing\") and not structured_data.get(\"incoming\")):\n",
    "        instance_iri = structured_data.get(\"instance_iri\", \"Unknown Instance\")\n",
    "        instance_name = clean_uri_for_llm_value(instance_iri)\n",
    "        return f\"name: {instance_name}\\n(No description data found)\"\n",
    "\n",
    "    instance_iri = structured_data.get(\"instance_iri\")\n",
    "    instance_name_cleaned = clean_uri_for_llm_value(instance_iri)\n",
    "\n",
    "    output_lines_part1 = []\n",
    "    output_lines_part2 = []\n",
    "\n",
    "    # --- Part 1: Outgoing Properties (key: value) ---\n",
    "    outgoing_properties = structured_data.get(\"outgoing\", {})\n",
    "    primary_name_val = instance_name_cleaned\n",
    "\n",
    "    temp_outgoing_formatted = {}\n",
    "    # Use the modified clean_uri_for_llm_key here\n",
    "    for pred_uri in sorted(outgoing_properties.keys()):\n",
    "        llm_key = clean_uri_for_llm_key(pred_uri)  # Uses new cleaning logic\n",
    "        values = outgoing_properties[pred_uri]\n",
    "        cleaned_values_for_key = []\n",
    "        for term_data in values:\n",
    "            cleaned_val = format_rdf_term_for_llm_value(term_data)\n",
    "            if cleaned_val and cleaned_val not in cleaned_values_for_key:\n",
    "                cleaned_values_for_key.append(cleaned_val)\n",
    "        if cleaned_values_for_key:\n",
    "            value_string = \", \".join(sorted(cleaned_values_for_key))\n",
    "            temp_outgoing_formatted[llm_key] = value_string\n",
    "            if llm_key == 'name':  # Check against the cleaned key 'name'\n",
    "                primary_name_val = value_string\n",
    "\n",
    "    # Generate output lines for part 1\n",
    "    if 'name' in temp_outgoing_formatted:\n",
    "        output_lines_part1.append(f\"name: {temp_outgoing_formatted['name']}\")\n",
    "    elif instance_name_cleaned:\n",
    "        output_lines_part1.append(f\"name: {instance_name_cleaned}\")\n",
    "\n",
    "    for key in sorted(temp_outgoing_formatted.keys()):\n",
    "        if key == 'name':\n",
    "            continue\n",
    "        output_lines_part1.append(f\"{key}: {temp_outgoing_formatted[key]}\")\n",
    "\n",
    "    # --- Part 2: Incoming Relationships (Subject : Predicate : Object) ---\n",
    "    incoming_relationships = structured_data.get(\"incoming\", {})\n",
    "    instance_name_for_part2 = primary_name_val\n",
    "\n",
    "    incoming_tuples = []\n",
    "    for pred_uri, subjects in incoming_relationships.items():\n",
    "        # **MODIFICATION**: Apply new cleaning logic to predicate for Part 2 output\n",
    "        # Extract local name first\n",
    "        if '#' in pred_uri:\n",
    "            pred_local_name = pred_uri.split('#')[-1]\n",
    "        else:\n",
    "            pred_local_name = pred_uri.split('/')[-1]\n",
    "        # Apply split and lowercase logic\n",
    "        pred_cleaned_for_output = split_camel_case_to_lower_words(\n",
    "            pred_local_name)\n",
    "\n",
    "        for subj_uri in subjects:\n",
    "            cleaned_subj = clean_uri_for_llm_value(subj_uri)\n",
    "            if cleaned_subj:\n",
    "                incoming_tuples.append(\n",
    "                    (cleaned_subj, pred_cleaned_for_output, instance_name_for_part2))\n",
    "\n",
    "    # Sort the tuples primarily by subject, then predicate\n",
    "    incoming_tuples.sort()\n",
    "\n",
    "    # Generate output lines for part 2 from sorted tuples\n",
    "    for subj, pred, obj in incoming_tuples:\n",
    "        output_lines_part2.append(f\"{subj} : {pred} : {obj}\")\n",
    "\n",
    "    # --- Combine Output ---\n",
    "    final_output = \"\\n\".join(output_lines_part1)\n",
    "    if output_lines_part2:\n",
    "        if output_lines_part1:\n",
    "            final_output += \"\\n\\n\"\n",
    "        final_output += \"\\n\".join(output_lines_part2)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "# Step 3: Describe a single instance\n",
    "def describe_instance(instance_iri):\n",
    "    \"\"\"\n",
    "    Fetches DESCRIBE data, processes it structurally, and then formats\n",
    "    it into the custom two-part layout for LLM input (revised key/predicate format).\n",
    "    \"\"\"\n",
    "    sparql = get_sparql()\n",
    "    sparql.setReturnFormat(N3)\n",
    "    query = f\"DESCRIBE <{instance_iri}>\"\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    try:\n",
    "        result_bytes = sparql.query().convert()\n",
    "        rdf_n3_string = result_bytes.decode('utf-8')\n",
    "\n",
    "        structured_data = extract_structured_description(\n",
    "            rdf_n3_string, instance_iri)\n",
    "\n",
    "        if structured_data is None:\n",
    "            raise ValueError(\"Failed to parse RDF data.\")\n",
    "\n",
    "        llm_input_string = format_for_llm_custom_layout(structured_data)\n",
    "        return llm_input_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Describing {instance_iri}: {e}\")\n",
    "        try:\n",
    "            folder_location = os.path.join(os.getcwd(), \"datasets\", \"failed\")\n",
    "            os.makedirs(folder_location, exist_ok=True)  # ensure 'data/' exists\n",
    "            failed_instance_file = os.path.join(folder_location, \"failed_instance_iri.txt\")\n",
    "            with open(failed_instance_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(instance_iri + \"\\n\")\n",
    "            print(f\"Logged failed IRI to {failed_instance_file}\")\n",
    "        except Exception as file_err:\n",
    "            print(f\"[Error] Saving failed IRI: {file_err}\")\n",
    "        return None\n",
    "\n",
    "# Step 4: Threaded orchestration\n",
    "def process_class(ontology_class):\n",
    "    instances = fetch_instances_for_class(ontology_class)\n",
    "    for iri in instances:\n",
    "        describe_instance_str = describe_instance(iri)\n",
    "        print(f\"\\n[DESCRIBE] {iri}\\n{describe_instance_str}\\n\")\n",
    "\n",
    "# Step 5: Main runner with threading\n",
    "def main():\n",
    "    owl_classes = fetch_classes()\n",
    "    print(f\"Total owl classes fetched: {len(owl_classes)}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        futures = [executor.submit(process_class, owl_class)\n",
    "                   for owl_class in owl_classes]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[Error in thread result] {e}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the following information about the entity http://dbpedia.org/resource/100_Word_Story:\n",
    "\n",
    "# Name: 100 Word Story\n",
    "# Abbreviation: 100 Word Story\n",
    "# Type: Academic Journal, Periodical Literature, Written Work, Creative Work\n",
    "# First published in: 2011\n",
    "# Frequency of publication: Quarterly\n",
    "# Academic discipline: Literary Magazine\n",
    "# Editor: Grant Faulkner\n",
    "# Homepage: http://www.100wordstory.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql():\n",
    "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "    return sparql\n",
    "\n",
    "# **MODIFIED FUNCTION**\n",
    "def split_camel_case_to_lower_words(name):\n",
    "    \"\"\"Splits CamelCase or PascalCase and returns lowercase words separated by spaces.\"\"\"\n",
    "    if not name: return \"\"\n",
    "    # Handle simple cases first\n",
    "    if name.islower() or '_' in name or not re.search('[A-Z]', name):\n",
    "        return name.replace('_', ' ').lower() # Replace underscores and lowercase\n",
    "\n",
    "    # Insert space before uppercase letters (except at the start)\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', name)\n",
    "    # Insert space before uppercase letters that follow lowercase or digit\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', s1)\n",
    "    return s2.lower() # Convert the whole result to lowercase\n",
    "\n",
    "# **MODIFIED FUNCTION**\n",
    "def clean_uri_for_llm_key(uri_str):\n",
    "    \"\"\"Cleans a predicate URI string into a readable key (lowercase, space-separated).\"\"\"\n",
    "    if not uri_str: return \"unknown property\"\n",
    "\n",
    "    # Specific overrides first (already lowercase)\n",
    "    if uri_str == str(RDF.type): return \"type\"\n",
    "    if uri_str == str(RDFS.label): return \"label\"\n",
    "    if uri_str == str(FOAF.name): return \"name\"\n",
    "    # Add other specific overrides if needed\n",
    "\n",
    "    # General cleaning - extract local name\n",
    "    if '#' in uri_str: name = uri_str.split('#')[-1]\n",
    "    else: name = uri_str.split('/')[-1]\n",
    "\n",
    "    # Split camel case and convert to lowercase words\n",
    "    return split_camel_case_to_lower_words(name)\n",
    "\n",
    "# (Other helper functions remain the same)\n",
    "def clean_uri_for_llm_value(uri_str):\n",
    "    if not uri_str: return \"Unknown Resource\"\n",
    "    if '#' in uri_str: name = uri_str.split('#')[-1]\n",
    "    else: name = uri_str.split('/')[-1]\n",
    "    name = name.replace('%28', '(').replace('%29', ')')\n",
    "    if uri_str.startswith(\"http://www.wikidata.org/entity/\"):\n",
    "        return name # Just return QID like Q215380\n",
    "    return name.replace('_', ' ')\n",
    "\n",
    "def format_rdf_term_for_llm_value(term_data):\n",
    "    if isinstance(term_data, dict): # Literal dictionary\n",
    "        val = term_data.get(\"value\", \"\")\n",
    "        val = re.sub(r'@\\w+$', '', val)\n",
    "        val = re.sub(r'\\^\\^<.*>$', '', val)\n",
    "        val = val.strip('\"')\n",
    "        return val\n",
    "    elif isinstance(term_data, str): # URI string\n",
    "        return clean_uri_for_llm_value(term_data)\n",
    "    else:\n",
    "        return str(term_data)\n",
    "\n",
    "# --- Step 1: Extract Structured Data (Unchanged Predicates) ---\n",
    "# (This function remains exactly the same as in the previous answers)\n",
    "def format_rdf_term(term):\n",
    "    # ... (same as before) ...\n",
    "    if isinstance(term, Literal):\n",
    "        dt = str(term.datatype) if term.datatype else None\n",
    "        if dt is None and term.language: dt = str(RDF.langString)\n",
    "        elif dt is None: dt = str(XSD.string)\n",
    "        return {\"value\": str(term), \"language\": term.language, \"datatype\": dt}\n",
    "    elif isinstance(term, URIRef): return str(term)\n",
    "    else: return str(term)\n",
    "\n",
    "def extract_structured_description(rdf_n3_string, instance_iri):\n",
    "    # ... (same as before) ...\n",
    "    if not rdf_n3_string:\n",
    "        return {\"instance_iri\": instance_iri, \"outgoing\": {}, \"incoming\": {}}\n",
    "    g = Graph()\n",
    "    try:\n",
    "        g.parse(data=rdf_n3_string, format=\"n3\", publicID=instance_iri)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Parsing N3 data for {instance_iri}: {e}\")\n",
    "        return None\n",
    "    instance_ref = URIRef(instance_iri)\n",
    "    outgoing_data = defaultdict(list)\n",
    "    incoming_data = defaultdict(list)\n",
    "    # Outgoing\n",
    "    for pred, obj in g.predicate_objects(subject=instance_ref):\n",
    "        pred_uri_str = str(pred)\n",
    "        formatted_obj = format_rdf_term(obj)\n",
    "        if formatted_obj not in outgoing_data[pred_uri_str]:\n",
    "             outgoing_data[pred_uri_str].append(formatted_obj)\n",
    "    # Incoming\n",
    "    for subj, pred in g.subject_predicates(object=instance_ref):\n",
    "        if subj == instance_ref: continue\n",
    "        pred_uri_str = str(pred)\n",
    "        subj_uri_str = str(subj)\n",
    "        incoming_data[pred_uri_str].append(subj_uri_str)\n",
    "    # Final Structure\n",
    "    final_outgoing = {pred: sorted(values, key=str) for pred, values in outgoing_data.items()}\n",
    "    final_incoming = {pred: values for pred, values in incoming_data.items()}\n",
    "    return {\"instance_iri\": instance_iri, \"outgoing\": final_outgoing, \"incoming\": final_incoming}\n",
    "\n",
    "\n",
    "# --- Step 2: Format Structured Data into Custom Layout for LLM (Revised Keys/Predicates) ---\n",
    "\n",
    "def format_for_llm_custom_layout(structured_data):\n",
    "    \"\"\"\n",
    "    Takes the structured dictionary and formats it into the specific\n",
    "    two-part layout requested by the user (revised key/predicate format).\n",
    "\n",
    "    Args:\n",
    "        structured_data: The dictionary produced by extract_structured_description.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string for LLM input in the custom layout.\n",
    "    \"\"\"\n",
    "    if not structured_data or (not structured_data.get(\"outgoing\") and not structured_data.get(\"incoming\")):\n",
    "        instance_iri = structured_data.get(\"instance_iri\", \"Unknown Instance\")\n",
    "        instance_name = clean_uri_for_llm_value(instance_iri)\n",
    "        return f\"name: {instance_name}\\n(No description data found)\"\n",
    "\n",
    "    instance_iri = structured_data.get(\"instance_iri\")\n",
    "    instance_name_cleaned = clean_uri_for_llm_value(instance_iri)\n",
    "\n",
    "    output_lines_part1 = []\n",
    "    output_lines_part2 = []\n",
    "\n",
    "    # --- Part 1: Outgoing Properties (key: value) ---\n",
    "    outgoing_properties = structured_data.get(\"outgoing\", {})\n",
    "    primary_name_val = instance_name_cleaned\n",
    "\n",
    "    temp_outgoing_formatted = {}\n",
    "    # Use the modified clean_uri_for_llm_key here\n",
    "    for pred_uri in sorted(outgoing_properties.keys()):\n",
    "        llm_key = clean_uri_for_llm_key(pred_uri) # Uses new cleaning logic\n",
    "        values = outgoing_properties[pred_uri]\n",
    "        cleaned_values_for_key = []\n",
    "        for term_data in values:\n",
    "            cleaned_val = format_rdf_term_for_llm_value(term_data)\n",
    "            if cleaned_val and cleaned_val not in cleaned_values_for_key:\n",
    "                cleaned_values_for_key.append(cleaned_val)\n",
    "        if cleaned_values_for_key:\n",
    "            value_string = \", \".join(sorted(cleaned_values_for_key))\n",
    "            temp_outgoing_formatted[llm_key] = value_string\n",
    "            if llm_key == 'name': # Check against the cleaned key 'name'\n",
    "                primary_name_val = value_string\n",
    "\n",
    "    # Generate output lines for part 1\n",
    "    if 'name' in temp_outgoing_formatted:\n",
    "         output_lines_part1.append(f\"name: {temp_outgoing_formatted['name']}\")\n",
    "    elif instance_name_cleaned:\n",
    "         output_lines_part1.append(f\"name: {instance_name_cleaned}\")\n",
    "\n",
    "    for key in sorted(temp_outgoing_formatted.keys()):\n",
    "        if key == 'name': continue\n",
    "        output_lines_part1.append(f\"{key}: {temp_outgoing_formatted[key]}\")\n",
    "\n",
    "\n",
    "    # --- Part 2: Incoming Relationships (Subject : Predicate : Object) ---\n",
    "    incoming_relationships = structured_data.get(\"incoming\", {})\n",
    "    instance_name_for_part2 = primary_name_val\n",
    "\n",
    "    incoming_tuples = []\n",
    "    for pred_uri, subjects in incoming_relationships.items():\n",
    "        # **MODIFICATION**: Apply new cleaning logic to predicate for Part 2 output\n",
    "        # Extract local name first\n",
    "        if '#' in pred_uri: pred_local_name = pred_uri.split('#')[-1]\n",
    "        else: pred_local_name = pred_uri.split('/')[-1]\n",
    "        # Apply split and lowercase logic\n",
    "        pred_cleaned_for_output = split_camel_case_to_lower_words(pred_local_name)\n",
    "\n",
    "        for subj_uri in subjects:\n",
    "            cleaned_subj = clean_uri_for_llm_value(subj_uri)\n",
    "            if cleaned_subj:\n",
    "                incoming_tuples.append((cleaned_subj, pred_cleaned_for_output, instance_name_for_part2))\n",
    "\n",
    "    # Sort the tuples primarily by subject, then predicate\n",
    "    incoming_tuples.sort()\n",
    "\n",
    "    # Generate output lines for part 2 from sorted tuples\n",
    "    for subj, pred, obj in incoming_tuples:\n",
    "         output_lines_part2.append(f\"{subj} : {pred} : {obj}\")\n",
    "\n",
    "\n",
    "    # --- Combine Output ---\n",
    "    final_output = \"\\n\".join(output_lines_part1)\n",
    "    if output_lines_part2:\n",
    "        if output_lines_part1:\n",
    "            final_output += \"\\n\\n\"\n",
    "        final_output += \"\\n\".join(output_lines_part2)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "# --- Main Execution Logic --- (Unchanged)\n",
    "\n",
    "def describe_instance_custom_layout(instance_iri):\n",
    "    \"\"\"\n",
    "    Fetches DESCRIBE data, processes it structurally, and then formats\n",
    "    it into the custom two-part layout for LLM input (revised key/predicate format).\n",
    "    \"\"\"\n",
    "    sparql = get_sparql()\n",
    "    sparql.setReturnFormat(N3)\n",
    "    query = f\"DESCRIBE <{instance_iri}>\"\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    try:\n",
    "        result_bytes = sparql.query().convert()\n",
    "        rdf_n3_string = result_bytes.decode('utf-8')\n",
    "\n",
    "        structured_data = extract_structured_description(rdf_n3_string, instance_iri)\n",
    "\n",
    "        if structured_data is None:\n",
    "             raise ValueError(\"Failed to parse RDF data.\")\n",
    "\n",
    "        llm_input_string = format_for_llm_custom_layout(structured_data)\n",
    "        return llm_input_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Describing or formatting for LLM {instance_iri}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- (http://dbpedia.org/resource/!!!) ---\n",
      "name: !!!\n",
      "active years start year: 1996\n",
      "alias: Chk Chk Chk\n",
      "background: group_or_band\n",
      "band member: Nic Offer\n",
      "former band member: Jerry Fuchs, Justin Van Der Volgen\n",
      "genre: Alternative dance, Dance-punk, Disco-rock, Funk rock, Indietronica\n",
      "hometown: California, Sacramento, California\n",
      "record label: Gold Standard Laboratories, Touch and Go Records, Warp (record label)\n",
      "type: Agent, Band, Group, MusicGroup, Organisation, Organization, Q215380, Q24229398, Q43229, SocialPerson, Thing\n",
      "\n",
      " One Boy : artist : !!!\n",
      "Chris Coady : associated band : !!!\n",
      "Chris Coady : associated musical artist : !!!\n",
      "Jerry Fuchs : associated band : !!!\n",
      "Jerry Fuchs : associated musical artist : !!!\n",
      "Maserati (band) : associated band : !!!\n",
      "Maserati (band) : associated musical artist : !!!\n",
      "Nic Offer  Nic Offer  1 : associated band : !!!\n",
      "Nic Offer  Nic Offer  1 : associated musical artist : !!!\n",
      "Raleigh Moncrief : associated band : !!!\n",
      "Raleigh Moncrief : associated musical artist : !!!\n",
      "The Juan MacLean : associated band : !!!\n",
      "The Juan MacLean : associated musical artist : !!!\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- (http://dbpedia.org/resource/100_Word_Story) ---\n",
      "name: 100 Word Story\n",
      "abbreviation: 100 Word Story\n",
      "academic discipline: Literary magazine\n",
      "editor: Grant Faulkner\n",
      "first publication year: 2011\n",
      "frequency of publication: Quarterly\n",
      "type: AcademicJournal, CreativeWork, PeriodicalLiterature, Q1092563, Q234460, Q386724, Thing, Work, WrittenWork\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage --- (Unchanged)\n",
    "if __name__ == \"__main__\":\n",
    "    test_iri_band = \"http://dbpedia.org/resource/!!!\"\n",
    "    llm_input_band = describe_instance_custom_layout(test_iri_band)\n",
    "\n",
    "    if llm_input_band:\n",
    "        print(f\"\\n--- ({test_iri_band}) ---\")\n",
    "        print(llm_input_band)\n",
    "    else:\n",
    "        print(f\"\\nFailed to get or format description for {test_iri_band}\")\n",
    "\n",
    "    print(\"\\n----------------------------------------\")\n",
    "\n",
    "    test_iri_journal = \"http://dbpedia.org/resource/100_Word_Story\"\n",
    "    llm_input_journal = describe_instance_custom_layout(test_iri_journal)\n",
    "    if llm_input_journal:\n",
    "         print(f\"\\n--- ({test_iri_journal}) ---\")\n",
    "         print(llm_input_journal)\n",
    "    else:\n",
    "         print(f\"\\nFailed to get or format description for {test_iri_journal}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ollama-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
