{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import OllamaLLM, ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "import psycopg\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, POST, N3\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, OWL, FOAF, XSD, SKOS, DCTERMS\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import urllib.parse\n",
    "from typing import List, Tuple, Dict, Set, DefaultDict\n",
    "import asyncio\n",
    "from asyncio import Lock, Semaphore\n",
    "import aiohttp\n",
    "import logging\n",
    "import aiofiles\n",
    "import json\n",
    "from asyncio import Lock\n",
    "from more_itertools import chunked\n",
    "import nest_asyncio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)  # Change to WARNING or ERROR in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langain & Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check ollama status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl --location 'http://127.0.0.1:11434/api/generate' \\\n",
    "# --header 'Content-Type: application/json' \\\n",
    "# --data '{ \\\n",
    "#     \"model\": \"llama3.2:3b\", \\\n",
    "#     \"prompt\": \"hello llama!\",  \\\n",
    "#     \"options\": { \\\n",
    "#         \"temperature\": 0 \\\n",
    "#     } \\\n",
    "# }' \\\n",
    "# | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://localhost:11434/api/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama model: configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0)\n",
    "# llm = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.2:3b', temperature=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "chat_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: invoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# llm.invoke(input=\"tell me a joke\")\n",
    "response = llm.invoke(\"hello ollama!\")\n",
    "\n",
    "# response = llm.invoke(\"Create an agent that uses Ollama function calling in Langchain.\")\n",
    "\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "chat_ollama.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = chat_prompt_template | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing llm: chat prompt template & StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt_template | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vector store & a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. select a specfic datasource. In this case a web page.\n",
    "# 2. save extracted content from the web page as docs.\n",
    "# 3. index the docs using FAISS vector store.\n",
    "# 4. convert the vector store to retriever.\n",
    "\n",
    "web_base_loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = web_base_loader.load()\n",
    "\n",
    "# print(f\"type(docs) : {type(docs)} \\n\")\n",
    "# print(f\"len(docs) : {len(docs)}\\n\")\n",
    "# print(f\"docs: {docs} \\n\")\n",
    "# type(f\"docs[0] : {docs[0]} \\n\")\n",
    "# print(f\"docs[0].page_content : {docs[0].page_content} \\n\")\n",
    "\n",
    "recursive_character_text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = recursive_character_text_splitter.split_documents(documents=docs)\n",
    "\n",
    "\n",
    "# print(type(documents))\n",
    "# print(len(documents))\n",
    "# print(documents)\n",
    "# print(documents[0])\n",
    "# print(documents[2])\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents, embedding=ollama_embedding)\n",
    "\n",
    "\n",
    "# print(f\"vector_store.index.ntotal: {vector_store.index.ntotal}\")\n",
    "# print(f\"vector_store._get_retriever_tags() : {vector_store._get_retriever_tags()}\")\n",
    "# print(f\"vector_store.index_to_docstore_id : {vector_store.index_to_docstore_id}\")\n",
    "# print(f\"type(vector_store.index_to_docstore_id) : {type(vector_store.index_to_docstore_id)}\")\n",
    "\n",
    "vector_store_retriever = vector_store.as_retriever()\n",
    "print(f\"vector_store_retriever: {vector_store_retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create a chat prompt template\n",
    "# 6. create a stuff document chain that accepts a llm model and chat prompt template & we can also run stuff document chain by passing in documents directly\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(\n",
    "    llm=llm, prompt=chat_prompt_template)\n",
    "response = documents_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"how can langsmith help with testing?\",\n",
    "        \"context\": documents\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. create a document retrieval chain that takes vector store retriever and stuff document chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    vector_store_retriever, documents_chain)\n",
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "# print(type(response))\n",
    "pprint.pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "history_aware_retriever_chain = create_history_aware_retriever(\n",
    "    llm, vector_store_retriever, chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, chat_prompt_template)\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    history_aware_retriever_chain, document_chain)\n",
    "\n",
    "chat_history = [HumanMessage(\n",
    "    content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"tell me how\"\n",
    "})\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_embedding = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")\n",
    "# ollama_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "ollama_embedding = OllamaEmbeddings(model=\"bge-m3:567m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: postgresql+psycopg2://user:password@host:port/dbname\n",
    "# Database Connection Details\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "COLLECTION_NAME = \"dbpedia_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Connecting to PGVector 'dbpedia_docs'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfull!\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nConnecting to PGVector '{COLLECTION_NAME}'...\")\n",
    "try:\n",
    "    # If the collection table doesn't exist, PGVector will try to create it.\n",
    "    vectorstore = PGVector(\n",
    "        connection=CONNECTION_STRING,\n",
    "        embeddings=ollama_embedding,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        use_jsonb=True\n",
    "        # pre_delete_collection=True\n",
    "        # Use pre_delete_collection=True if you want to clear the collection on every run (USE WITH CAUTION!)\n",
    "        # pre_delete_collection=False,\n",
    "    )\n",
    "    print(f\"connection successfull!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    logger.exception(f\"\\nDatabase Connection Error: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.exception(f\"\\nAn error occurred during PGVector connection: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the entity Description|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_FILENAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred while reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Example Usage:\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m read_one_jsonl(\u001b[43mOUTPUT_FILENAME\u001b[49m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miri\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OUTPUT_FILENAME' is not defined"
     ]
    }
   ],
   "source": [
    "def read_one_jsonl(filename):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file line by line and yields each parsed JSON object.\n",
    "    This allows processing one record at a time without loading the whole file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_number, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    yield json.loads(line) # Yield the parsed dictionary\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON on line {line_number} in {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filename}: {e}\")\n",
    "\n",
    "# Example Usage:\n",
    "for record in read_one_jsonl(OUTPUT_FILENAME):\n",
    "    print(\"---\"*50)\n",
    "    print(f\"IRI: {record.get('iri')}\")\n",
    "    print(f\"Description: {record.get('description')}\")\n",
    "    # Process the record here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the following information about the entity http://dbpedia.org/resource/100_Word_Story:\n",
    "\n",
    "# Name: 100 Word Story\n",
    "# Abbreviation: 100 Word Story\n",
    "# Type: Academic Journal, Periodical Literature, Written Work, Creative Work\n",
    "# First published in: 2011\n",
    "# Frequency of publication: Quarterly\n",
    "# Academic discipline: Literary Magazine\n",
    "# Editor: Grant Faulkner\n",
    "# Homepage: http://www.100wordstory.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ollama-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
